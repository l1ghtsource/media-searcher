# Лидеры Цифровой Трансформации 2024

*MISIS ITUT ITAM team*

Team Members:
1) **Егор Чистов** - Backend
2) **Дмитрий Коноплянников** - Frontend
3) **Анна Гулякина** - Design
4) **Кирилл Рыжичкин** - ML Engineer
5) **Артём Плужников** - ML Engineer

Презентация: [link](https://www.google.com/)

Веб-сервис: [itutitam.ru](https://www.google.com/)

API: [itutitam.ru/api/](https://www.google.com/)

## Кейс "Сервис текстового поиска по медиаконтенту" (Yappy)

> В библиотеке Yappy десятки миллионов коротких видео. Возможность быстро и эффективно находить интересующий контент улучшает пользовательский опыт, помогает найти новые интересы пользователя и улучшить рекомендации. Разработайте сервис, позволяющий индексировать и осуществлять поиск по видео на основе медиаконтента. Сервис должен уметь обрабатывать запросы пользователей, извлекать из них ключевые слова и на их основе осуществлять поиск релевантных видеофайлов.

## Предложенное решение

### Принцип работы системы изображен на блок-схеме: 

<тут будет блок-схема>

### Для извлечения фичей из видео использовались следующие подходы:
1) __CLIP__ (а именно *Searchium-ai/clip4clip-webvid150k*, обученный на парах "поисковый запрос - видео") - строим эмбеддинги видео (пространство этих эмбеддингов устроено так, что в нём векторные представления близких текстов и видео тоже близки, то есть эмбеддинги слова "кот" и видео с котом будут иметь высокие показатели близости, а это именно то, что нам и надо)
![scheme_clip](clip_scheme.png)
3) __OCR + Text Embedding__ - извлекаем текст из видео с помощью *EasyOCR* (берём фреймы каждые 3 секунды, далее считываем с них текст, чтение текста по фреймам распараллелено, это позволило значительно повысить скорость обработки одного видео), далее строим эмбеддинг текста с помощью *all-MiniLM-L6-v2*, это позволяет работать с видео, характерными для формата Yappy и Tiktok - когда на фоне просто нарезка каких-то фрагментов из игр или фильмов, а основная информация содержится в тексте, наложенном на видео
![scheme_ocr](easyocr_scheme.jpeg)
5) __VAD + ASR + Text Embedding__ - используем *silero-vad* для нахождения фрагментов аудиодорожки видео с голосом, транскрибируем с помощью *whisper-medium*, далее строим эмбеддинг текста с помощью *all-MiniLM-L6-v2*, это позволяет работать с видео, в которых основная информация содержится в их аудиосоставляющей. Более того, в том случае, когда в аудио вместо речи играет музыка, whisper возвращает "МУЗЫКА", а если аплодисменты, то "АПЛОДИСМЕНТЫ" и т.д, то есть данная модель устойчива к различным видам аудио.
![scheme_asr](whisper_scheme.png)

### Замеры скорости на видео длительностью 25 секунд:

<div align="center">
   
| Модель          | Время GPU (s) | Время CPU (s) |
|-----------------|-----------------|-----------------|
| [clip_model.py](https://github.com/l1ghtsource/media-searcher/blob/main/ml/clip_model.py)   | 4.3             | 7.5             |
| [whisper_model.py](https://github.com/l1ghtsource/media-searcher/blob/main/ml/whisper_model.py)| 2.68 (medium)   | 6 (base)        |
| [ocr_model.py](https://github.com/l1ghtsource/media-searcher/blob/main/ml/ocr_model.py)    | 2.23            | 21              |
| [meta_model.py](https://github.com/l1ghtsource/media-searcher/blob/main/ml/meta_model.py) (объединяет в себя все прошлые пункты)   | 8               | 32              |

Изначально *OCRModel* считывал текст с фреймов последовательно, а в *MetaModel* происходило распараллеливание *CLIPModel*, *OCRModel* и *WhisperModel*. Однако, когда мы начали проводить замеры скорости на CPU, было обнаружено, что если распараллелить именно процесс считывания текста с фреймов в *OCRModel*, то скорость значительно возрастет. То есть сначала на все возможные ресурсы параллелится *OCRModel*, а когда считываение текста завершится уже *CLIPModel* и *WhisperModel* распределяют ресурсы между собой. Это ускорило инференс на CPU с одной минуты до 20 секунд!

</div>

### Обработка поискового запроса:
1) __CLIP Embedding__ - строим с помощью *clip4clip-webvid150k* векторное представление поискового запроса для мэтчинга с CLIP-эмбеддингами видео
2) __Text Embedding__ - строим с помощью *all-MiniLM-L6-v2* векторное представление поискового запроса для мэтчинга с эмбеддингами видео (векторными представлениями текстов, полученных с OCR и ASR)
3) __Ranking__ - на основании полученных эмбеддингов переведенного поискового запроса и эмбеддингов видео, содержащихся в БД *Clickhouse*, проводим быстрый поиск top-k похожих видео с помощью *faiss*, также здесь автоматически рассчитываются веса OCR, ASR и CLIP, на основании количества слов, распознанных *Whisper* и *EasyOCR*

### Замеры скорости:

<div align="center">
   
| Модель          | Время GPU (ms) | Время CPU (ms) |
|-----------------|-----------------|-----------------|
| [text2clip_model.py](https://github.com/l1ghtsource/media-searcher/blob/main/ml/text2clip_model.py) (построение эмбеддинга)| 17           | 50             |
| [text2minilm_model.py](https://github.com/l1ghtsource/media-searcher/blob/main/ml/text2minilm_model.py) (построение эмбеддинга)| 8   | 22        |
| [ranker.py](https://github.com/l1ghtsource/media-searcher/blob/main/ml/ranker.py)<br>(включая перевод, построение эмбеддингов и сам поиск top-k при k=20)| 211               | 323              |

</div>

## Кластеризация или поиск видео по тематикам

Имея значительное количество строк в БД с эмбеддингами видео, захотелось провести их кластеризацию. К эмбеддингам *CLIP* были применены стандартизация, *PCA* с 50 главными компонентами, затем был использован алгоритм *hdbscan* непосредственно для кластеризации. Было получено 98 различных кластеров, содержащих внутри себя видео одной тематики, дальше кластерам были даны названия и была проведена их фильтрация. В итоге осталось 48 качественных кластеров. Таким образом, у пользователя помимо опции обычного поиска подходящих видео по его запросу появилась опция выбрать определенный кластер, внутри которого содержатся точно отобранные видео на одну определённую тематику, соответствующую названию кластера.

Для удобного добавления видео были рассчитаны средние эмбеддинги для каждого кластера, при загрузке видео мы добавляем его в наиболее подходящий кластер, основываясь на значении косинусной близости CLIP-эмбеддинга загруженного видео со средним CLIP-эмбеддингом всего кластера.

Код кластеризации: [clusterization.py](https://github.com/l1ghtsource/media-searcher/blob/main/ml/clusterization_by_themes.py)

Полученные кластеры: [final_clusters.csv](https://github.com/l1ghtsource/media-searcher/blob/main/data/final_clusters.csv)

## Fine-tuning переводчика на собственном датасете

Перед нами встала проблема: использовать API Google Translator и других популярных переводчиков запрещено, а open-source решения, к удивлению, не справляются с простыми запросами. Например, на запрос "роблокс", все популярные модели с huggingface выдавали результат "robls" или "roblocks", а эмбеддинг этого слова в CLIP-пространстве был далек от непосредственно роблокса и близок к блокам, что портило поисковую выдачу. На запрос "райан гослинг" маленькими буквами модель вовсе давала ответ "I'll take care of you. I'll take care of you". Поэтому, из популярных под видео тэгов Yappy, а также некоторых добавленных слов был [собран](https://github.com/l1ghtsource/media-searcher/blob/main/utils/generate_dataset_for_translator_tuning.py) небольшой датасет из 10000 русских слов, далее эти слова были переведены с помощью Google Translator на английский и на полученных парах слов была [дообучена](https://github.com/l1ghtsource/media-searcher/blob/main/ml/translator_train.py) модель машинного перевода *Helsinki-NLP/opus-mt-en-ru*. Модель выбирали исходя из [бенчмарков](https://huggingface.co/spaces/utrobinmv/TREX_benchmark_en_ru_zh) по соотношению скорости и качества ru-en перевода.

Модель: https://huggingface.co/lightsource/yappy-fine-tuned-opus-mt-ru-en

Исходная модель для сравнения: https://huggingface.co/Helsinki-NLP/opus-mt-ru-en

## Кластеризация видео по действующим лицам

Также у нас появилась идея - кластеризовать видео по лицам, участвующих в них. Для этого использовали *opencv* и *face_recognition*, мы находим уникальные лица на видео и возвращаем эмбеддинг самого часто встречающегося лица. На обработку 1000 видео на CPU уходило примерно 3 часа, на GPU не завелось :(

Полученные 31000 эмбеддингов кластеризовали с *PCA* и *hdbscan*, получили 138 кластеров, из них отобрали 80 качественных.

Среднее время обработки одного видео:

<div align="center">

| Модель          | Время GPU (s) | Время CPU (s) |
|-----------------|-----------------|-----------------|
| [face_founder_model.py](https://github.com/l1ghtsource/media-searcher/blob/main/ml/face_founder_model.py) | -           | 10             |

</div>

Код кластеризации: [clusterization_by_faces.py](https://github.com/l1ghtsource/media-searcher/blob/main/ml/clusterization_by_faces.py)

Полученные кластеры: [faces-31000-videos-filtered.csv](https://github.com/l1ghtsource/media-searcher/blob/main/data/faces-31000-videos-filtered.csv)

## Kaggle ноутбуки

<div align="center">

| Ноутбук                                           | Описание                                     |
|---------------------------------------------------|----------------------------------------------|
| [Ноутбук с OCR, ASR, CLIP, Ranker, кластеризацией по тематикам и замерами скорости](https://www.kaggle.com/code/l1ghtsource/yappy-hackathon/) | Включает основные классы и замеры времени на GPU и CPU. |
| [Ноутбук с файн-тюнингом переводчика](https://www.kaggle.com/code/l1ghtsource/transaltor-finetune/) | Содержит файн-тюнинг переводчика на собственном датасете. |
| [Ноутбук с кластеризацией видео по лицам](https://www.kaggle.com/code/l1ghtsource/yappy-faces-clustering/) | Содержит код для кластеризации видео по лицам. |

</div>

### Пример поиска видео:

```python
import clickhouse_connect
from ml.ranker import SimilarityRanker

client = clickhouse_connect.get_client(host='91.224.86.248', port=8123) # подключение к БД, содержащей предрасчитанные эмбеддинги
TABLENAME = 'embeddings'

data = client.query_df(f'SELECT id, clip_emb, ocr_emb, whisper_emb, whisper_len, ocr_len FROM {TABLENAME}')
data = data.drop_duplicates(subset='id')

df = pd.read_csv('path-to-csv-with-links') # исходный CSV-файл, содержайщий ссылки на видео

ranker = SimilarityRanker(data, df)
res = ranker.find_top_k('бравл старс', k=10)
```

## Преимущества решения:
1) Быстрый поиск подходящих видео с помощью *faiss* (~300ms на CPU, ~200ms на GPU для топ-20 видео)
2) Извлекаем максимум информации: текст с видео (*EasyOCR*, причем извлекаем текст с фреймов параллельно, что даёт сильный прирост в скорости), текстовое представление аудиосоставляющей (*Whisper*), учитываем происходящее на видео (CLIP)
3) Автоматическое взвешивание суммы близостей по всем трём эмбеддингам в зависимости от количества распознанных слов *Whisper* и *EasyOCR* (то есть если *Whisper*'ом слов будет распознано мало - веса сместится на *CLIP* и *OCR*, и наоборот)
4) Возможность сортировки видео по тематикам (кластеризация по CLIP-эмбеддингам)
5) Возможность сортировки видео по персоналиям (будут найдены видео с конкретным человеком)
6) Собственный переводчик, зафайнтюненный на корпусе из популярных в Yappy слов (верно интерпретируем слэнг-запросы)
7) Мультиязычность (решение не зависит ни от языка видео, ни от языка поискового запроса)
8) Быстрое автопродолжение поискового запроса и автокорреция орфографических ошибок [TODO]
9) Скорость построения новых эмбеддингов и занесения их в *Clickhouse*-хранилище, на GPU за 5-15с будут получены транскрибация, текст с видео, построены три эмбеддинга и занесены в БД
10) Real-time голосовой ввод в поле поиска
11) Независимость от тегов и описания видео (работаем только с медиа-контентом)

## Нереализованные идеи

1. Поиск видео по тональности (весёлые, грустные, смешные и т.д.)
   - [kaggle ноутбук с наработками](https://www.kaggle.com/code/l1ghtsource/yappy-sentiment/)
   - сначала попытались построить CLIP-эмбеддинги слов "sad", 'funny" и пр., далее рассчитать косинусную близость CLIP-эмбеддинга каждого видео с получеными векторами слов, далее с помощью softmax получили "вероятности" принадлежности видео каждому из классов, однако качество данной классификации нас не устроило
   - также была идея провести sentiment analysis по ocr и whisper эмбеддингам, для этого нашли модель (https://huggingface.co/shhossain/all-MiniLM-L6-v2-sentiment-classifier), работающую ровно с тем типом эмбеддингов, который мы используем, однако классы, на которых училась эта модель не совсем нам подходили, поэтому тоже решили отказаться от этой идеи (времени собрать датасет и обучить свою подобную модель уже не оставалось)
